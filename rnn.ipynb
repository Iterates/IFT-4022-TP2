{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #2 : Classification d'incidents avec un réseau  récurrent et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette tâche est similaire à la précédente et vous réutilisez les mêmes fichiers d’entraînement, de validation et de test. Cependant, vous devez utiliser des réseaux récurrents pour classifier les textes. Plus particulièrement, vous devez entraîner un réseau de neurones LSTM pour encoder les textes et une couche linéaire pour faire la classification des textes. \n",
    "\n",
    "Les consignes pour cette tâche sont: \n",
    "- \tNom du notebook : rnn.ipynb\n",
    "- \tTokenisation : Utilisation de Spacy. \n",
    "- \tPlongements de mots : Ceux de Spacy. \n",
    "- \tNormalisation : Aucune normalisation. \n",
    "- \tStructure du réseau : Un réseau LSTM avec 1 seule couche pour l’encodage de textes. Je vous laisse déterminer la taille de cette couche (à expliquer). \n",
    "- \tAnalyse : Comparer les résultats obtenus avec un réseau unidirectionnel et un réseau bidirectionnel. Si vous éprouvez des difficultés à entraîner les 2 réseaux dans un même notebook, faites une copie et nommez le 2e fichier rnn-bidirectionnel.ipynb.\n",
    "- \tExpliquez comment les modèles sont utilisés pour faire la classification d’un texte. \n",
    "- \tPrésentez clairement vos résultats et faites-en l’analyse. \n",
    "\n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela améliore la lisibilité.\n",
    "\n",
    "Notes :\n",
    "- Évitez les bouts de code trop longs ou trop complexes. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriquées. Si c'est le cas, définissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement d'un modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load(filename: str):\n",
    "    dataset = {}\n",
    "    with open(filename, \"r\") as fp:\n",
    "        incident_list = json.load(fp)\n",
    "    texts, labels = zip(*[(incident.get(\"text\"), int(incident.get(\"label\"))) for incident in incident_list])\n",
    "    dataset[\"text\"] = list(texts)\n",
    "    dataset[\"label\"] = list(labels)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load('./data/incidents_train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Conversion en Identifiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = list(set(dataset[\"label\"]))\n",
    "id2label = {int(label): label for label in labels}\n",
    "label2id = {label: int(label) for label in labels}\n",
    "nb_labels = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "model = spacy.load('en_core_web_md')\n",
    "\n",
    "embedding_size = model.meta['vectors']['width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "class Tokens(Enum):\n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "\n",
    "zero_vec_embedding = np.zeros(embedding_size, dtype=np.float64)\n",
    "\n",
    "id2word = {}\n",
    "id2word[0] = Tokens.PADDING\n",
    "id2word[1] = Tokens.UNKNOWN\n",
    "\n",
    "word2id = {}\n",
    "word2id[Tokens.PADDING] = 0\n",
    "word2id[Tokens.UNKNOWN] = 1\n",
    "\n",
    "id2embedding = {}\n",
    "id2embedding[0] = zero_vec_embedding\n",
    "id2embedding[1] = zero_vec_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ids():\n",
    "    word_index = 2\n",
    "    vocabulary = word2id.keys()\n",
    "\n",
    "    for sentence in dataset[\"text\"]:\n",
    "        for word in model(sentence):\n",
    "            if word.text not in vocabulary:\n",
    "                word2id[word.text] = word_index\n",
    "                id2word[word_index] = word.text\n",
    "                id2embedding[word_index] = word.vector\n",
    "                word_index += 1\n",
    "\n",
    "map_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import LongTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, data , targets, word_to_id, spacy_model):\n",
    "        self.data = data\n",
    "        self.sequences = [None for _ in range(len(data))]\n",
    "        self.targets = targets\n",
    "        self.word2id = word_to_id\n",
    "        self.tokenizer = spacy_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sequences[index] is None:\n",
    "            self.sequences[index] = self.tokenize(self.data[index])\n",
    "        \n",
    "        return LongTensor(self.sequences[index]), LongTensor([self.targets[index]]).squeeze(0)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        tokens = [word.text for word in self.tokenizer(sentence)]\n",
    "        \n",
    "        return [self.word2id.get(token, 1) for token in tokens] \n",
    "\n",
    "\n",
    "train_dataset = QuestionDataset(dataset['text'], dataset['label'], word2id, model)\n",
    "\n",
    "dataset = load('./data/incidents_dev.json')\n",
    "\n",
    "validation_dataset = QuestionDataset(dataset['text'], dataset['label'], word2id, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Rnn(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_state_size, nb_classes, bidirectional=False):\n",
    "        super(Rnn, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.embedding_size = embeddings.size()[1]\n",
    "        self.lstm = nn.LSTM(self.embedding_size, hidden_state_size, 1, batch_first=True, bidirectional=bidirectional)\n",
    "        self.classification_layer = nn.Linear(hidden_state_size * 2 if bidirectional else hidden_state_size, nb_classes)\n",
    "        self.hidden_state = None\n",
    "        self.context = None\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embedding_layer(x)\n",
    "        packed_batch = pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (self.hidden_state, self.context) = self.lstm(packed_batch)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            x = torch.cat((self.hidden_state[0], self.hidden_state[1]), 1)\n",
    "            x = self.classification_layer(x)\n",
    "        else:\n",
    "            x = self.hidden_state.squeeze()\n",
    "            x = self.classification_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    x = [x for x,y in batch]\n",
    "    x_true_length = [len(x) for x,y in batch]\n",
    "    y = torch.stack([y for x,y in batch], dim=0)\n",
    "    return ((pad_sequence(x, batch_first=True), x_true_length), y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(id2embedding)\n",
    "\n",
    "embedding_layer = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
    "\n",
    "for token_id, embedding in id2embedding.items():\n",
    "    embedding_layer[token_id,:] = embedding\n",
    "    \n",
    "embedding_layer = torch.from_numpy(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne import set_seeds\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "hidden_size = 100 \n",
    "\n",
    "network = Rnn(embedding_layer, hidden_size, nb_labels, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_bundle(network, name, epochs=5):\n",
    "    # \"\"\"\n",
    "    # This function creates a Poutyne ModelBundle, trains the input module\n",
    "    # on the train loader and then tests its performance on the test loader.\n",
    "    # All training and testing statistics are saved, as well as best model\n",
    "    # checkpoints.\n",
    "# \n",
    "    # Args:\n",
    "        # network (torch.nn.Module): The neural network to train.\n",
    "        # working_directory (str): The directory where to output files to save.\n",
    "        # epochs (int): The number of epochs. (Default: 5)\n",
    "    # \"\"\"\n",
    "    # print(network)\n",
    "# \n",
    "    # optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "# \n",
    "    # save_path = os.path.join('saves', name)\n",
    "# \n",
    "    # model_bundle = ModelBundle.from_network(\n",
    "        # save_path,\n",
    "        # network,\n",
    "        # device=device,\n",
    "        # optimizer=optimizer,\n",
    "        # task='classif',\n",
    "    # )\n",
    "# \n",
    "    # model_bundle.train(train_loader, valid_loader, epochs=epochs)\n",
    "# \n",
    "    # model_bundle.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from model/lstm_classification/checkpoint.ckpt and starting at epoch 26.\n",
      "Loading optimizer state from model/lstm_classification/checkpoint.optim and starting at epoch 26.\n",
      "Loading random states from model/lstm_classification/checkpoint.randomstate and starting at epoch 26.\n",
      "Restoring data from model/lstm_classification/checkpoint_epoch_21.ckpt\n"
     ]
    }
   ],
   "source": [
    "from poutyne.framework import Experiment\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), learning_rate)\n",
    "\n",
    "model_directory = 'model/lstm_classification_bidirectional/' if network.bidirectional else 'model/lstm_classification/'\n",
    "\n",
    "experiment = Experiment(model_directory, network, optimizer=optimizer, task=\"classification\", loss_function=cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging = experiment.train(train_dataloader, validation_dataloader, epochs=3, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load('./data/incidents_test.json')\n",
    "\n",
    "test_dataset = QuestionDataset(dataset['text'], dataset['label'], word2id, model)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best checkpoint at epoch: 21\n",
      "lr: 0.1, loss: 0.125539, acc: 98.3051, fscore_macro: 0.924582, val_loss: 0.10593, val_acc: 99.2467, val_fscore_macro: 0.930552\n",
      "Loading checkpoint model/lstm_classification/checkpoint_epoch_21.ckpt\n",
      "Running test\n",
      "\u001b[35mTest steps: \u001b[36m34 \u001b[32m2.63s \u001b[35mtest_loss:\u001b[94m 2.026285\u001b[35m test_acc:\u001b[94m 49.529190\u001b[35m test_fscore_macro:\u001b[94m 0.279179\u001b[0m          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 2.6329579999437556,\n",
       " 'test_loss': 2.026285367412756,\n",
       " 'test_acc': 49.52919020715631,\n",
       " 'test_fscore_macro': 0.2791789174079895}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
