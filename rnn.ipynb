{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #2 : Classification d'incidents avec un réseau  récurrent et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette tâche est similaire à la précédente et vous réutilisez les mêmes fichiers d’entraînement, de validation et de test. Cependant, vous devez utiliser des réseaux récurrents pour classifier les textes. Plus particulièrement, vous devez entraîner un réseau de neurones LSTM pour encoder les textes et une couche linéaire pour faire la classification des textes. \n",
    "\n",
    "Les consignes pour cette tâche sont: \n",
    "- \tNom du notebook : rnn.ipynb\n",
    "- \tTokenisation : Utilisation de Spacy. \n",
    "- \tPlongements de mots : Ceux de Spacy. \n",
    "- \tNormalisation : Aucune normalisation. \n",
    "- \tStructure du réseau : Un réseau LSTM avec 1 seule couche pour l’encodage de textes. Je vous laisse déterminer la taille de cette couche (à expliquer). \n",
    "- \tAnalyse : Comparer les résultats obtenus avec un réseau unidirectionnel et un réseau bidirectionnel. Si vous éprouvez des difficultés à entraîner les 2 réseaux dans un même notebook, faites une copie et nommez le 2e fichier rnn-bidirectionnel.ipynb.\n",
    "- \tExpliquez comment les modèles sont utilisés pour faire la classification d’un texte. \n",
    "- \tPrésentez clairement vos résultats et faites-en l’analyse. \n",
    "\n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela améliore la lisibilité.\n",
    "\n",
    "Notes :\n",
    "- Évitez les bouts de code trop longs ou trop complexes. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriquées. Si c'est le cas, définissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement d'un modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load(filename: str):\n",
    "    dataset = {}\n",
    "    with open(filename, \"r\") as fp:\n",
    "        incident_list = json.load(fp)\n",
    "    texts, labels = zip(*[(incident.get(\"text\"), int(incident.get(\"label\"))) for incident in incident_list])\n",
    "    dataset[\"text\"] = list(texts)\n",
    "    dataset[\"label\"] = list(labels)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load('./data/incidents_train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Conversion en Identifiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = list(set(dataset[\"label\"]))\n",
    "id2label = {int(label): label for label in labels}\n",
    "label2id = {label: int(label) for label in labels}\n",
    "nb_labels = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "model = spacy.load('en_core_web_md')\n",
    "\n",
    "embedding_size = model.meta['vectors']['width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "class Tokens(Enum):\n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "\n",
    "zero_vec_embedding = np.zeros(embedding_size, dtype=np.float64)\n",
    "\n",
    "id2word = {}\n",
    "id2word[0] = Tokens.PADDING\n",
    "id2word[1] = Tokens.UNKNOWN\n",
    "\n",
    "word2id = {}\n",
    "word2id[Tokens.PADDING] = 0\n",
    "word2id[Tokens.UNKNOWN] = 1\n",
    "\n",
    "id2embedding = {}\n",
    "id2embedding[0] = zero_vec_embedding\n",
    "id2embedding[1] = zero_vec_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ids():\n",
    "    word_index = 2\n",
    "    vocabulary = word2id.keys()\n",
    "\n",
    "    for sentence in dataset[\"text\"]:\n",
    "        for word in model(sentence):\n",
    "            if word.text not in vocabulary:\n",
    "                word2id[word.text] = word_index\n",
    "                id2word[word_index] = word.text\n",
    "                id2embedding[word_index] = word.vector\n",
    "                word_index += 1\n",
    "\n",
    "map_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import LongTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, data , targets, word_to_id, spacy_model):\n",
    "        self.data = data\n",
    "        self.sequences = [None for _ in range(len(data))]\n",
    "        self.targets = targets\n",
    "        self.word2id = word_to_id\n",
    "        self.tokenizer = spacy_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sequences[index] is None:\n",
    "            self.sequences[index] = self.tokenize(self.data[index])\n",
    "        \n",
    "        return LongTensor(self.sequences[index]), LongTensor([self.targets[index]]).squeeze(0)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        tokens = [word.text for word in self.tokenizer(sentence)]\n",
    "        \n",
    "        return [self.word2id.get(token, 1) for token in tokens] \n",
    "\n",
    "\n",
    "train_dataset = QuestionDataset(dataset['text'], dataset['label'], word2id, model)\n",
    "\n",
    "dataset = load('./data/incidents_dev.json')\n",
    "\n",
    "validation_dataset = QuestionDataset(dataset['text'], dataset['label'], word2id, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  2, 107,   8, 108,   2, 109,   2,  11,  12,  13,   2,  16, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118,   2, 119, 120,  14, 121, 122,  32,\n",
       "         123, 124,  16, 125, 126,  26,  66, 127,  32,  30, 128,   2, 129,  87,\n",
       "         130, 131, 132,  53, 133, 134,  26,  66, 128, 129, 127,  14,   2, 135,\n",
       "          30, 136,  32,  30, 129, 137,  56, 138,  30, 133,  34, 115,   2,  30,\n",
       "         136,  26,  11,  12,  13, 139,  14, 140,  30, 141,  32,  30, 142,  87,\n",
       "          16, 143,  32,  30,   2, 144, 145, 146,  30, 147, 148, 149, 150,  53,\n",
       "          30, 136, 151,  14,   2, 152, 115, 153, 154,  26,  11,  12,  13,  14,\n",
       "         155,  30,  83,   2, 151,  14,   2, 121, 156,  32,  30, 147, 148, 149,\n",
       "         157,  30, 136, 138,  30, 158,   2, 134,  26,  11,  12,  13,  14, 159,\n",
       "          62,  30, 160,  26,  66, 161, 140,   2, 162,  48, 163,  51, 164, 165,\n",
       "           7,  59,  32,  30, 166, 167, 168, 151,  51,   2,   4, 169, 112, 170,\n",
       "         171,  62, 172,  26, 173]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Rnn(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_state_size, nb_classes, bidirectional=False):\n",
    "        super(Rnn, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.embedding_size = embeddings.size()[1]\n",
    "        self.lstm = nn.LSTM(self.embedding_size, hidden_state_size, 1, batch_first=True, bidirectional=bidirectional)\n",
    "        self.classification_layer = nn.Linear(hidden_state_size * 2 if bidirectional else hidden_state_size, nb_classes)\n",
    "        self.hidden_state = None\n",
    "        self.context = None\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embedding_layer(x)\n",
    "        packed_batch = pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, (self.hidden_state, self.context) = self.lstm(packed_batch)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            output, _ = pad_packed_sequence(output, batch_first=True) \n",
    "            x = output.squeeze()\n",
    "            x = self.classification_layer(output)\n",
    "        else:\n",
    "            x = self.hidden_state.squeeze()\n",
    "            x = self.classification_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    x = [x for x,y in batch]\n",
    "    x_true_length = [len(x) for x,y in batch]\n",
    "    y = torch.stack([y for x,y in batch], dim=0)\n",
    "    return ((pad_sequence(x, batch_first=True), x_true_length), y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=True, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(id2embedding)\n",
    "\n",
    "embedding_layer = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
    "\n",
    "for token_id, embedding in id2embedding.items():\n",
    "    embedding_layer[token_id,:] = embedding\n",
    "    \n",
    "embedding_layer = torch.from_numpy(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poutyne import set_seeds\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "hidden_size = 100 \n",
    "\n",
    "model = Rnn(embedding_layer, hidden_size, nb_labels, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x100 and 200x9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ari\\Desktop\\IFT-4022\\tp2\\rnn.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ari/Desktop/IFT-4022/tp2/rnn.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), learning_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ari/Desktop/IFT-4022/tp2/rnn.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m experiment \u001b[39m=\u001b[39m Experiment(\u001b[39m'\u001b[39m\u001b[39mmodel/lstm_classification\u001b[39m\u001b[39m'\u001b[39m, model, optimizer\u001b[39m=\u001b[39moptimizer, task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m, loss_function\u001b[39m=\u001b[39mcross_entropy)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ari/Desktop/IFT-4022/tp2/rnn.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m logging \u001b[39m=\u001b[39m experiment\u001b[39m.\u001b[39mtrain(train_dataloader, validation_dataloader, epochs\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m, disable_tensorboard\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model_bundle.py:751\u001b[0m, in \u001b[0;36mModelBundle.train\u001b[1;34m(self, train_generator, valid_generator, **kwargs)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_generator, valid_generator\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict]:\n\u001b[0;32m    710\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[39m    Trains or finetunes the model on a dataset using a generator. If a previous training already occurred\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[39m    and lasted a total of `n_previous` epochs, then the model's weights will be set to the last checkpoint and the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[39m        List of dict containing the history of each epoch.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit_generator, train_generator, valid_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model_bundle.py:912\u001b[0m, in \u001b[0;36mModelBundle._train\u001b[1;34m(self, training_func, callbacks, lr_schedulers, keep_only_last_best, save_every_epoch, disable_tensorboard, seed, *args, **kwargs)\u001b[0m\n\u001b[0;32m    909\u001b[0m     expt_callbacks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m callbacks\n\u001b[0;32m    911\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 912\u001b[0m     \u001b[39mreturn\u001b[39;00m training_func(\u001b[39m*\u001b[39margs, initial_epoch\u001b[39m=\u001b[39minitial_epoch, callbacks\u001b[39m=\u001b[39mexpt_callbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    913\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogging:\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model.py:610\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, train_generator, valid_generator, epochs, steps_per_epoch, validation_steps, batches_per_step, initial_epoch, verbose, progress_options, callbacks)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_generator_n_batches_per_step(epoch_iterator, callback_list, batches_per_step)\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_generator_one_batch_per_step(epoch_iterator, callback_list)\n\u001b[0;32m    612\u001b[0m \u001b[39mreturn\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39mepoch_logs\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model.py:681\u001b[0m, in \u001b[0;36mModel._fit_generator_one_batch_per_step\u001b[1;34m(self, epoch_iterator, callback_list)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_training_mode(\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    680\u001b[0m     \u001b[39mfor\u001b[39;00m step, (x, y) \u001b[39min\u001b[39;00m train_step_iterator:\n\u001b[1;32m--> 681\u001b[0m         step\u001b[39m.\u001b[39mloss, step\u001b[39m.\u001b[39mbatch_metrics, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_batch(x, y, callback\u001b[39m=\u001b[39mcallback_list, step\u001b[39m=\u001b[39mstep\u001b[39m.\u001b[39mnumber)\n\u001b[0;32m    682\u001b[0m         step\u001b[39m.\u001b[39msize \u001b[39m=\u001b[39m get_batch_size(x, y)\n\u001b[0;32m    684\u001b[0m train_step_iterator\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_loss()\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model.py:693\u001b[0m, in \u001b[0;36mModel._fit_batch\u001b[1;34m(self, x, y, callback, step, return_pred, convert_to_numpy)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_batch\u001b[39m(\u001b[39mself\u001b[39m, x, y, \u001b[39m*\u001b[39m, callback\u001b[39m=\u001b[39mCallback(), step\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, return_pred\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, convert_to_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    691\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 693\u001b[0m     loss_tensor, batch_metrics, pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_loss_and_metrics(\n\u001b[0;32m    694\u001b[0m         x, y, return_loss_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_pred\u001b[39m=\u001b[39mreturn_pred, convert_to_numpy\u001b[39m=\u001b[39mconvert_to_numpy\n\u001b[0;32m    695\u001b[0m     )\n\u001b[0;32m    697\u001b[0m     loss_tensor\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    698\u001b[0m     callback\u001b[39m.\u001b[39mon_backward_end(step)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\poutyne\\framework\\model.py:1477\u001b[0m, in \u001b[0;36mModel._compute_loss_and_metrics\u001b[1;34m(self, x, y, return_loss_tensor, return_pred, convert_to_numpy)\u001b[0m\n\u001b[0;32m   1475\u001b[0m     pred_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39mdata_parallel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork, x, [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mother_device)\n\u001b[0;32m   1476\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1477\u001b[0m     pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork(\u001b[39m*\u001b[39mx)\n\u001b[0;32m   1478\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(pred_y, y)\n\u001b[0;32m   1479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_loss_tensor:\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Ari\\Desktop\\IFT-4022\\tp2\\rnn.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ari/Desktop/IFT-4022/tp2/rnn.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_layer(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ari/Desktop/IFT-4022/tp2/rnn.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ari/Desktop/IFT-4022/tp2/rnn.ipynb#X36sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_layer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ari/Desktop/IFT-4022/tp2/rnn.ipynb#X36sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x100 and 200x9)"
     ]
    }
   ],
   "source": [
    "from poutyne.framework import Experiment\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "experiment = Experiment('model/lstm_classification', model, optimizer=optimizer, task=\"classification\", loss_function=cross_entropy)\n",
    "\n",
    "logging = experiment.train(train_dataloader, validation_dataloader, epochs=25, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
