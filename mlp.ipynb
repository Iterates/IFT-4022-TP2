{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tâche #1 : Classification d'incidents avec un réseau *feedforward* et des *embeddings* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend la classification des descriptions d’accidents du premier travail. Le corpus de textes contient 3 partitions : \n",
    "-\tun fichier d’entraînement -  data/incidents_train.json\n",
    "-\tun fichier de validation -  data/incidents_dev.json\n",
    "-\tun fichier de test - data/incidents_test.json\n",
    "\n",
    "Entraînez un modèle de réseau de neurones de type feedforward multicouche (MLP) avec plongements de mots pour déterminer le type d’un incident à partir de sa description. \n",
    "\n",
    "Voici les consignes pour cette tâche : \n",
    "\n",
    "-\tNom du notebook : mlp.ipynb\n",
    "-\tTokenisation : Utilisation de Spacy. \n",
    "-\tPlongements de mots : Ceux de Spacy. \n",
    "-\tNormalisation : Aucune normalisation. \n",
    "-\tAgrégation des plongements de mots : Comparer les approches max, average et min pooling. \n",
    "-\tStructure du réseau : 1 seule couche cachée dont vous choisirez la taille (à expliquer). \n",
    "-\tPrésentez clairement vos résultats et faites-en l’analyse. En cas de doute, inspirez-vous de ce qui a été fait dans le travail pratique #1. \n",
    "\n",
    "Vous pouvez ajouter au *notebook* toutes les cellules dont vous avez besoin pour votre code, vos explications ou la présentation de vos résultats. Vous pouvez également ajouter des sous-sections (par ex. des sous-sections 1.1, 1.2 etc.) si cela améliore la lisibilité.\n",
    "\n",
    "Notes :\n",
    "- Évitez les bouts de code trop longs ou trop complexes. Par exemple, il est difficile de comprendre 4-5 boucles ou conditions imbriquées. Si c'est le cas, définissez des sous-fonctions pour refactoriser et simplifier votre code. \n",
    "- Expliquez sommairement votre démarche.\n",
    "- Expliquez les choix que vous faites au niveau de la programmation et des modèles (si non trivial).\n",
    "- Analyser vos résultats. Indiquez ce que vous observez, si c'est bon ou non, si c'est surprenant, etc. \n",
    "- Une analyse quantitative et qualitative d'erreurs est intéressante et permet de mieux comprendre le comportement d'un modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création du jeu de données (*dataset*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from typing import List, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import List, Callable\n",
    "from spacy import Language\n",
    "from torch import nn\n",
    "from poutyne.framework import Experiment\n",
    "from poutyne import set_seeds\n",
    "from torch.optim import SGD\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "def load(filename: str):\n",
    "    with open(filename, \"r\") as fp:\n",
    "        incident_list = json.load(fp)\n",
    "    texts, targets = zip(\n",
    "        *[\n",
    "            (incident.get(\"text\"), int(incident.get(\"label\")))\n",
    "            for incident in incident_list\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return list(texts), np.array(targets, dtype=np.uint8)\n",
    "\n",
    "\n",
    "class SpacyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: List[str],\n",
    "        target: np.array,\n",
    "        sentence_aggregation_function: Callable[[List[str], Language], np.array] = None,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.doc_embeddings = [None for _ in range(len(dataset))]\n",
    "        self._sentence_aggregation_function = sentence_aggregation_function\n",
    "        self._target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.doc_embeddings[index] is None:\n",
    "            self.doc_embeddings[index] = self.sentence_aggregation_function(\n",
    "                self.dataset[index]\n",
    "            )\n",
    "        return FloatTensor(self.doc_embeddings[index]), LongTensor(\n",
    "            [self.target[index]]\n",
    "        ).squeeze(0)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataset)\n",
    "\n",
    "    @property\n",
    "    def sentence_aggregation_function(self):\n",
    "        return self._sentence_aggregation_function\n",
    "\n",
    "    @sentence_aggregation_function.setter\n",
    "    def sentence_aggregation_function(\n",
    "        self, function: Callable[[List[str], Language], np.array]\n",
    "    ):\n",
    "        if not callable(function):\n",
    "            raise ValueError(\"Parameter must be of function type\")\n",
    "        self._sentence_aggregation_function = function\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        return self._target\n",
    "\n",
    "    @property\n",
    "    def nb_classes(self):\n",
    "        return len(set(self._target))\n",
    "\n",
    "    @property\n",
    "    def target_categories(self):\n",
    "        return list(set(self._target))\n",
    "\n",
    "\n",
    "texts, targets = load(\"./data/incidents_dev.json\")\n",
    "dev = SpacyDataset(texts, targets)\n",
    "texts, targets = load(\"./data/incidents_train.json\")\n",
    "train = SpacyDataset(texts, targets)\n",
    "texts, targets = load(\"./data/incidents_test.json\")\n",
    "test = SpacyDataset(texts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gestion de plongements de mots (*embeddings*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "def average_embedding(sentence, nlp_model=nlp):\n",
    "    tokenised_sentence = nlp_model(sentence)\n",
    "    nb_column = len(tokenised_sentence)\n",
    "    nb_rows = nlp_model.vocab.vectors_length\n",
    "    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))\n",
    "    for index, token in enumerate(tokenised_sentence):\n",
    "        sentence_embedding_matrix[:, index] = token.vector\n",
    "    return np.average(sentence_embedding_matrix, axis=1)\n",
    "\n",
    "\n",
    "def maxpool_embedding(sentence, nlp_model=nlp):\n",
    "    tokenised_sentence = nlp_model(sentence)\n",
    "    nb_column = len(tokenised_sentence)\n",
    "    nb_rows = nlp_model.vocab.vectors_length\n",
    "    sentence_embedding_matrix = np.zeros((nb_rows, nb_column))\n",
    "    for index, token in enumerate(tokenised_sentence):\n",
    "        sentence_embedding_matrix[:, index] = token.vector\n",
    "    return np.max(sentence_embedding_matrix, axis=1)\n",
    "\n",
    "\n",
    "def spacy_embedding(sentence, nlp_model=nlp):\n",
    "    tokenised_sentence = nlp_model(sentence)\n",
    "    return tokenised_sentence.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explications\n",
    "\n",
    "### Choix de la taille de la couche cachée \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Création de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=16, shuffle=True)\n",
    "\n",
    "train.sentence_aggregation_function = spacy_embedding\n",
    "dev.sentence_aggregation_function = spacy_embedding\n",
    "test.sentence_aggregation_function = spacy_embedding\n",
    "\n",
    "embedding_size = nlp.meta[\"vectors\"][\"width\"]\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.intput_layer = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.output_layer = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.intput_layer(x)\n",
    "        x = nn.functional.relu_(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "set_seeds(42)\n",
    "hidden_size = 100\n",
    "\n",
    "directory_name = f\"model/{spacy_embedding.__name__}_mlp\"\n",
    "\n",
    "model = MultiLayerPerceptron(embedding_size, hidden_size, train.nb_classes)\n",
    "experiment = Experiment(directory_name, model, optimizer=\"SGD\", task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires\n",
    "\n",
    "Vous pouvez mettre ici toutes les fonctions qui seront utiles pour les sections suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement de modèle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = experiment.train(\n",
    "    train_dataloader, dev_dataloader, epochs=30, disable_tensorboard=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation et analyse de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_probable_class(\n",
    "    target: int,\n",
    "    target_categories: List[str],\n",
    "    aggregation_function: Callable[[str], List[str]],\n",
    "    sentence: str,\n",
    "    model: MultiLayerPerceptron,\n",
    "):\n",
    "    vectorized_sentence = aggregation_function(sentence)\n",
    "    prediction = model(FloatTensor(vectorized_sentence).squeeze(0)).detach()\n",
    "    output = softmax(prediction, dim=0)\n",
    "    max_category_index = np.argmax(output)\n",
    "    max_category = target_categories[max_category_index]\n",
    "    print(\"\\nClassification de la phrase: \", sentence)\n",
    "    print(\"Sorties du réseau de neurones:\", prediction)\n",
    "    print(\"Valeurs obtenues après application de softmax:\", output)\n",
    "    print(\n",
    "        f\"Meilleure classe: {max_category} qui correspond en sortie à la neurone {max_category_index}\"\n",
    "    )\n",
    "    print(f\"Classe réelle : {target}\")\n",
    "\n",
    "    return int(max_category == target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, sentence in enumerate(test):\n",
    "        total += 1\n",
    "        correct += get_most_probable_class(\n",
    "            test.target[i],\n",
    "            test.target_categories,\n",
    "            test.sentence_aggregation_function,\n",
    "            sentence,\n",
    "            model,\n",
    "        )\n",
    "    print(correct, total)\n",
    "\n",
    "\n",
    "display(test.target_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
